{
    "contents" : "\n###### TD Analyse de données ######\n\nsetwd(\"~/R/MSIAAP\") # set working directory : permet de définir l'emplacement du répertoire de travail\n\nrm(list = ls () ) # permet de nettoyer l'espace de travail\n                        # rm(X) enlève l'objet X\n                        # rm(list = X) enlève la liste d'objets X\n\n# NB : x11() permet d'ouvrir une autre fenêtre graphique\n\nsource(\"Data/Fonctions_R/Dcentred.txt\")\nsource(\"Data/Fonctions_R/acpxqd.txt\")  \n\nrequire(ggplot2)\nrequire(plyr)\nrequire(dplyr)\nrequire(scales)\n\n\n                                ###___###\n\n\n\n##### ACP\n\n\n### Importation des données poissons\n\namiard <- read.table(\"Data/Fonctions_R/Amiard.txt\", header=F)\n\n# trivia : ls() permet de donner le contenu de l'espace de travail\n# trivia : dim(X) permet de donner les dimensions de l'objet X en nb lignes et nb colonnes\n\n## Matrice centrée\namiardC <- scale(amiard, center = TRUE, scale = FALSE)\nmean(amiardC[,1]) # calcul de la moyenne de la première variable\n## Matrice centrée réduite\namiardCr <- scale(amiard, center = TRUE, scale = TRUE) \nsd(amiardCr[,1]) # calcul de l'écart type de la première variable. ici, sd()\n# donne la valeur estimée de l'écart type [utilisation de (n-1) et non pas n]\n# trivia : summary(X) permet d'avoir un ... sommaire (qui l'eut cru) de X\n\n## Calcul de la moyenne et de l'écart type pour chaque variable de amiard\n# ?apply() : apply(X, MARGIN, FUN, ...)\napply(amiard, MARGIN = 2, FUN = mean) # MARGIN = 2 pour travailler sur les colonnes de la matrice\napply(amiard, MARGIN = 2, FUN = sd)\n\n# Création boxplot pour observation de la distribution des variables des trois jeux de données\n\npar(mfrow=c(1,3))\n\nboxplot(amiard) # jeu de données de base\nboxplot(amiardC) # jeu de données centré : moyenne = 0\nboxplot(amiardCr) # jeu de données centré réduit : moyenne = 0 et écart type = 1\n\npairs(amiardCr) # crée une matrice de corrélation linéaire\n                # difficilement lisible\n\n### Réalisation de l'ACP\n\n# acpxqd() a été appelée au début du script\n\nres.pca <- acpxqd(as.matrix(amiard), centrer = TRUE, cor = TRUE, k = 3) # centrage et réduction du jeu de données initial\n# dans une ACP, l'inertie sur des données centrées réduites doit être égale au nombre de variables\n# choix du nombre de composantes : Somme des valeurs propres = 80 % ; \"règle du bâton brisé\"\n# valeur propre > 1 car la combinaison linéaire des variables doit avoir une variance\n# supérieure à celle d'une variable du jeu d'origine (dont la variance est égale à 1 car jeu de données centré réduit)\n# Le cercle de corrélation peut être tracé car les variables sont réduites. Plus une variable\n# est proche du cercle, mieux elle est projetée sur les axes (et donc les combinaisons \n# linéaires) représentées\n\nnames(res.pca) # liste des sous objets contenus dans res.pca\n# res.pca$C : variables\n# res.pca$A : individus\n\n#____#\n\n# Comment visualiser en 3d\n\nrequire(rgl)\n?plot3d\nplot3d(res.pca$A[,1], res.pca$A[,2], res.pca$A[,3], xlim = c(-1, 1), ylim = c(-1, 1), zlim = c(-1, 1))\ntext3d(res.pca$A[,1], res.pca$A[,2], res.pca$A[,3], texts = 1:16)\n\n#_____#\n\n# Comment visualiser les groupes de poissons sur notre ACP ?\n# Il faut créer un vecteur pour stocker les groupes de poissons\n\ngp <- c(rep(1:2, each = 8), rep(3,7))\n\n#res.pca$C contient les composantes de l'acp\n\npar(mfrow = c(1,2))\n# Graph des individus\nplot(res.pca$C[,1], res.pca$C[,2], type = \"n\") # type = null donc graph vide\ntext(res.pca$C[,1], res.pca$C[,2], col = gp) # ajoute les numéros sur le graphe vide\nabline(v = 0, h = 0)# l'individu moyen est présent au centre : plus les individus s'éloignent\n# du centre, plus ils sont atypiques\n# Graph des variables\nplot(res.pca$A[,1], res.pca$A[,2], type = \"n\", xlim = c(-1, 1), ylim = c(-1, 1)) # type = null donc graph vide\ntext(res.pca$A[,1], res.pca$A[,2])\nabline(v = 0, h = 0)\n\nimage(t(as.matrix(amiardCr))) # vue d'ensemble des données en utilisant des couleurs\n# la transposée de la matrice permet de visualiser les individus et les variables dans le bon sens\n\n\n### Importation des données villes\n\nvilles <- read.table(\"Data/Fonctions_R/villes.csv\", header = TRUE, sep = \";\", dec = \".\", row.names = 1)\nhead(villes) # aperçu des premières lignes d'un fichier\n\nvillesCr <- scale(villes)\n\nres.pca2 <- acpxqd(as.matrix(villes), centrer = TRUE, cor = TRUE, k = 3) # centrage et réduction du jeu de données initial\n\ndf.var <- as.data.frame(res.pca2$A)\nangle <- seq(-pi, pi, length = 50) \ncircle <- data.frame(x = sin(angle), y = cos(angle)) \n\n# Création vecteur pour colorer différemment les différents types de variables\ncol.vec <- as.factor(c(1, rep(2,4), rep(3,3), 1, rep(3,5), rep(4,5)))\n\nggplot(data = df.var, aes(x = a1, y = a2)) +\n        theme(aspect.ratio=1) +\n        geom_text(data = df.var, aes(x = a1, y = a2, \n                      label = row.names(df.var), color = col.vec), size = 6.5) +\n        geom_path(aes(x, y), data = circle, colour=\"grey70\") +\n        geom_hline(yintercept = 0, colour = \"gray65\") +\n        geom_vline(xintercept = 0, colour = \"gray65\") +\n        ggtitle(\"ACPvilles : Projection des variables \") +\n        scale_color_discrete(name = \"Type de variable\", labels = \n                                     c(\"Population\", \"Géographique\", \"Socio-économique\", \"Politique\")) +\n        xlab(\"Dimension 1. 36 % de variance expliquée\") +\n        ylab(\"Dimension 2. 26 % de variance expliquée\")\n        \ndf.ind <- as.data.frame(res.pca2$C)\n\nggplot(data = df.ind, aes(x = c1, y = c2)) +\n        theme(aspect.ratio=1) +\n        geom_text(data = df.ind, aes(x = c1, y = c2, \n                                     label = row.names(df.ind)), size = 6.5) +\n        geom_hline(yintercept = 0, colour = \"gray65\") +\n        geom_vline(xintercept = 0, colour = \"gray65\") +\n        ggtitle(\"ACPvilles : Projection des individus \") +\n        xlab(\"Dimension 1. 36 % de variance expliquée\") +\n        ylab(\"Dimension 2. 26 % de variance expliquée\")\n\n# Valeurs très négatives : la population contribue fortement à la création de l'axe2\n# il y a des valeurs très négatives car les villes représentées sont des grosses villes.\n\n\n\n\n                                        ###___###\n\n\n\n#### CAH\n\n# Travailler sur les données centrées réduites pour s'affranchir des unités\n\n# Calcul des distances\nmatdist <- dist(villesCr) # de base, uniquement matrice triangulaire\n\n# Visualisation\nimage(as.matrix(matdist))\n\n\n\n                        ###___###\n\n\n### Clustering\n\n\n\n\n# CAH\n\nreshclust <- hclust(matdist, method = \"ward.D2\") # Il faut utiliser les distances au carré avec Ward :\n# method = \"ward.D2\"\nreshclust2 <- hclust(matdist^2, method = \"ward.D\") # Si on veut mettre la matrice de distance au carré\n\n# Visualisation\nplot(reshclust, hang = -1)\n\n# Création des groupes\nrescut4 <- as.factor(cutree(reshclust, k = 4)) # Donne un vecteur contenant les groupes pour chaque ville\ntable(rescut4)\nsort(rescut)\n\n\nggplot(data = df.ind, aes(x = c1, y = c2)) +\n        theme(aspect.ratio = 1) +\n        geom_text(data = df.ind, aes(x = c1, y = c2, \n                                     label = row.names(df.ind), color = rescut4), size = 6.5) +\n        geom_hline(yintercept = 0, colour = \"gray65\") +\n        geom_vline(xintercept = 0, colour = \"gray65\") +\n        ggtitle(\"ACPvilles : Projection des individus \") +\n        scale_color_discrete(name = \"Groupes de villes \\naprès clustering par Ward\", labels = \n                                     c(\"1\", \"2\", \"3\", \"4\")) +\n        xlab(\"Dimension 1. 36 % de variance expliquée\") +\n        ylab(\"Dimension 2. 26 % de variance expliquée\")\n\n# Autre méthode\n\nplot(res.pca2$C[,1], res.pca2$C[,2])\nabline(h=0,v=0)\ntext(res.pca2$C[,1], res.pca2$C[,2], labels= rownames(res.pca2$C))\nplot(res.pca2$C[,1], res.pca2$C[,2], type=\"n\")\ntext(res.pca2$C[,1], res.pca2$C[,2], labels= rownames(res.pca2$C), col=cutree(reshclust, k = 4))\n\n\n\n\n\n\n# k-means\n\nreskm <- kmeans(villesCr, centers = 2)\nnames(reskm)\nreskm$centers\nreskm$totss # somme des variances pour toutes les variables\nreskm$withinss # cumul des variances intra groupes pour chaque groupe : on cherche à la minimiser\nreskm$tot.withinss\n\n# Estimation du nombre de groupe optimal\n\n# Méthode 1\nenrW <- NULL\nfor (i in 2:15){\n        reskm <- kmeans(villesCr, centers = i)\n        enrW <- c(enrW, reskm$tot.withinss)\n}\nplot(2:15, enrW)\n\n# Méthode 2\nenrW <- rep(NA, 14)\nfor (i in 2:15){\n        reskm <- kmeans(villesCr, centers = i)\n        enrW[i-1] <- reskm$tot.withinss\n}\nplot(2:15, enrW)\n\n# Méthode 3\nplot(c(2, 15), c(0, 400), type =\"n\")\nfor (i in 2:15){\n        reskm <- kmeans(villesXR, centers = i)\n        points(i, reskm$tot.withinss) # points ajoute des points sur un graph\n}\n\n## Méthode 2 avec plusieurs itérations\n\nenrW <- matrix(NA, nrow = 14, ncol = 200) # Pour chaque groupe, 200 répétitions\nfor (i in 2:15){\n        for (j in 1:200){\n                reskm <- kmeans(villesCr, centers = i)\n                enrW[i-1, j] <- reskm$tot.withinss\n        }\n}\n\nboxplot(t(enrW)) # Aide au choix du groupe : arrêt après le dernier grand saut : ici 4 groupes\n#boxplot(t(enrW), axes = NULL)\n#boxplot(t(enrW), axes = FALSE)\n#box()\n#axis(1, at = 1:14, 2:15) #modification de l'axe des abscisses\n\nreskm <- kmeans(villesCr, centers = 4) # résultats dépendent de l'initialisation\nreskm$cluster\ntable(reskm$clus,rescut4) # croisement des résultats du k-means et de la CAH pour les comparer\n\n\n\n\n\n\n\n###############\n### Linear Discriminant Analysis\n################\n\nrequire(MASS)\ndata(crabs)\nhead(crabs)\n\n# Est-ce que les 5 variables quantitatives permettent de prédire les 4 groupes ?\n# Par le biais d'une combinaison linéaire de variables\n\n## vecteur des groupes\n\ngp <- rep(1:4, each = 50) # meth 1 ; fonctionne uniquement car données ordonnées\ngp <- paste(crabs[,1], crabs[,2], sep = \"\") # meth 2, plus intéressante car valable tout le temps\ngp <- as.factor(gp)\n\n# données centrées réduites\nXcr <- scale(crabs[,4:ncol(crabs)])\nhead(Xcr)\n\nboxplot(Xcr) # Visualisation, les données semblent bien centrées réduites\npairs(Xcr, col = gp) # Matrice de corrélation. Attribution d'une couleur à chaque niveau de facteur\n\nreslda <- lda(x = Xcr, grouping = gp) # recherche des axes où les gropes sont les plus \n# éloignés possibles tandis que les groupes sont les plus compacts possibles. cette fonction utilise W^(-1)\nnames(resda) # svd : racine carrée des valeurs propres (sqrt(lambda W^(-1) B))\n#Dans cours, c = Xa avec X = Xcr et a = reslda$scaling\n# Il y a 3 composants car nb axes = Nb groupes - 1\n\n#pouvoir discriminant\nlambda <- (3*reslda$svd^2/196)/(1+(3*reslda$svd^2/196))\nlambda\nk <- length(levels(gp)) # nb groupes\nn <- length(gp) # nb obs\n\nlambda <- ((k-1)*reslda$svd^2/(n-k))/(1+((k-1)*reslda$svd^2/(n-k))) # formule générale pour le pouvoir discriminant\n\npredlda <- predict(reslda)\npredlda$class # affectation de chq observation\nnames(predlda)\n?predict # Que fait predict ? Peu d'info\nclass(reslda) # On utilise predict sur un objet de class lde\n?predict.lda # plus d'informations\n\ntabloc <- table(predlda$class, gp) # Comparaison des prédictions avec les vrais groupes\nsum(diag(tabloc))/nrow(Xcr) # Pourcentage de bien classés\npredlda$post # Probabilités à posteriori d'appartenance de chq indiv à chq classe. Distance\n#de chq observation par rapport au centre de gravité de chq groupes\npredlda$post[2,] # exemple de proba pour un indiv mal classé\n\npredlda$x # coordonnées de chq indiv sur les axes\n\n# Visualiser graphiquement\ndf.ind <- as.data.frame(predlda$x)\n\nggplot(data = df.ind, aes(x = LD1, y = LD2)) +\n        geom_point(data = df.ind, aes(x = LD1, y = LD2, color = gp))\n\n# autre méthode\ncolmoi = c(\"blue\", \"lightblue\", \"orange\", \"yellow\")\nplot(predlda$x[,1], predlda$x[,2], col = colmoi[as.numeric(gp)])\npoints(predlda$x[,1], predlda$x[,2], col = colmoi[as.numeric(predlda$class)], pch =\"+\")\n\n# Validation croisée LOO ; de base dans la fonction lda\nresldaCV <- lda(x = Xcr, grouping = gp, CV = TRUE)\nresldaCV$class # affectation à posteriori des indiv n'ayant pas servi à construire le modèle\n\nsum(diag(table(resldaCV$class, gp)))/nrow(Xcr) # Pourcentage de bien classé ; pas trop de sur ajustement\n\n\n\n\n\n#######\n# PLS\n#######\n\n\nprostate <- read.table(\"Data/Fonctions_R/prostate.txt\", header= TRUE)\ndim(prostate) # Vérification de la dimension du fichier importé\nhead(prostate)\nsummary(prostate)\nstr(prostate)\nimage(abs(cor(prostate[,1:9])))\nround(cor(prostate[,1:9], 2))\nboxplot(prostate[,1:9], las = 2) # las = 2 permet de mettre les notations en vertical sur les axes\n\n# Il va falloir centrer réduire les données au vu des résultats\n\n### Prédiction par modèle linéaire multiple de lpsa\n\n?lm()\n\nres.lm <- lm(lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45, data = prostate)\nclass(res.lm)\nnames(res.lm) # fitted values = y chapeaux\nsummary(res.lm) # Par construction, la moyenne des résidus est égale à 0. l'intercept serait = 0 si données centrées réduites\n\nplot(res.lm)\ncor(res.lm$fitted.values, prostate$lpsa)^2 # = R²\nplot(res.lm$fitted.values, prostate$lpsa, type =\"n\")\ntext(res.lm$fitted.values, prostate$lps)\n\n# Ici, PLS ne s'imposerait pas forcément puisqu'on obtient un résultat.\n\n# Sourcer les fonctions\nsource(\"Data/Fonctions_R/Dcentred.txt\")\nsource(\"Data/Fonctions_R/Dcenter.txt\")\nsource(\"Data/Fonctions_R/pls.txt\")\nsource(\"Data/Fonctions_R/Dcp.txt\")\nsource(\"Data/Fonctions_R/Dvar.txt\")\nsource(\"Data/Fonctions_R/Dcentred.txt\")\nsource(\"Data/Fonctions_R/Dproj.txt\")\nsource(\"Data/Fonctions_R/WDop.txt\")\nsource(\"Data/Fonctions_R/VQop.txt\")\nsource(\"Data/Fonctions_R/invgene.txt\")\nsource(\"Data/Fonctions_R/plscv.txt\")\nsource(\"Data/Fonctions_R/plscv.plot.txt\")\n\nres.pls <- pls(X = as.matrix(prostate[,1:8]), Y = as.matrix(prostate[,9]), A = 8)\n# X matrice des variables explicatives et  Y matrice des variables a expliquer\n# Si X = Y, on réalise une ACP\n\n# Ici, une seule variable à expliquer donc u1 = y. u1 = f(t1) montre une prédiction correcte\nnames(res.pls)\n\n# Utilisation de la fonction PLSCV : cross validation (LOO) après pls\n\nres.plscv <- plscv(X = as.matrix(prostate[,1:8]), Y = as.matrix(prostate[,9]), A = 8)\n\nplscv.plot(res.plscv) # Affichage des plots PRESS\n\n\n## Fonction sous R pour la PLS\n\n# PLS + validation croisée avec séparation en 2 groupes\nres.plsr <- plsr(lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45, data = prostate, validation = \"CV\", segments = 2)\n\nres.plsr$validation$PRESS\nplot(1:8, res.plsr$validation$PRESS)\n\n\n\n\n\n\n########\n# SVM\n########\n\nrequire(kernlab)\nrequire(MASS)\ndata(crabs)\n\n# Prédiction du sexe à partir des variables. Ici uniquement 2 groupes donc SVM s'applique facilement\n\ndatasvm <- crabs[, c(2, 4:8)]\n\nres.svm <- ksvm(sex ~ ., data = datasvm, type = \"C-svc\", kernel = \"rbfdot\")\n# type = \"C-svc\" car on veut faire de la classification. on a le choix entre la correction C et nu\n# kernel = \"rbfdot\" utilisation d'un noyau gaussien\n\npred.svm <- predict(res.svm) # stocke les 200 prédictions pour le sexe\n# ?predict.ksvm() Il est possible d'appliquer le modèle sur un jeu en précisant newdata =\ntable(pred.svm, crabs$sex)\n\n\n#### 2-CV\n\npred.CV <- matrix(NA, nrow = 200, ncol = 100)\nfor (i in 1:2){\n        for (j in 1:100){\n                reskm <- kmeans(villesCr, centers = i)\n                enrW[i-1, j] <- reskm$tot.withinss\n                res.svm <- ksvm(sex ~ ., data = datasvm[-1,], type = \"C-svc\", kernel = \"rbfdot\")\n                pred.svm <- predict(res.svm, newdata = datasvm[1,])\n        }\n}\n\n\npred.CV <- matrix(NA, nrow = 200, ncol = 100)\npbc <- rep(NA, 100)\n\nfor (j in 1:100)\n        {\n        ID <- sample(1:nrow(datasvm), round((nrow(datasvm))/2))\n\n        res.svm <- ksvm(sex ~ ., data = datasvm[-ID, ], type = \"C-svc\", kernel = \"rbfdot\")\n        pred.svm <- predict(res.svm, newdata = datasvm[ID, ])\n\n        pred.CV[ID, j] <- pred.svm # on a construit le modèle sans ID, donc on prédit ID\n\n        res.svm <- ksvm(sex ~ ., data = datasvm[ID, ], type = \"C-svc\", kernel = \"rbfdot\")\n        pred.svm <- predict(res.svm, newdata = datasvm[-ID, ])\n\n\n        pred.CV[- ID, j] <- pred.svm\n        \n        \n        pbc[j] <- sum(diag(table(pred.CV[, j], datasvm$sex)))/nrow(datasvm) #pourcentage de prédiction pour chaque modèle\n        \n}\nmean(pbc)\n\n## Optimisation du paramètre pour la correction de l'erreur, C\n\nseqC <- 10^((-2):2) # Séquences de puissances de 10 à tester pour C\n\nenrmoyen = NULL\nfor (i in seqC)\n        {\n        \n        pred.CV <- matrix(NA, nrow = 200, ncol = 30)\n        pbc <- rep(NA, 30)\n        \n        for (j in 1:30)\n                {\n                ID <- sample(1:nrow(datasvm), round((nrow(datasvm))/2))\n                \n                res.svm <- ksvm(sex ~ ., data = datasvm[-ID, ], type = \"C-svc\", C = i, kernel = \"rbfdot\")\n                pred.svm <- predict(res.svm, newdata = datasvm[ID, ])\n                \n                pred.CV[ID, j] <- pred.svm # on a construit le modèle sans ID, donc on prédit ID\n                \n                res.svm <- ksvm(sex ~ ., data = datasvm[ID, ], type = \"C-svc\", C = i, kernel = \"rbfdot\")\n                pred.svm <- predict(res.svm, newdata = datasvm[-ID, ])\n                \n                \n                pred.CV[- ID, j] <- pred.svm\n                \n                \n                pbc[j] <- sum(diag(table(pred.CV[, j], datasvm$sex)))/nrow(datasvm) #pourcentage de prédiction pour chaque modèle\n                \n        }\n        enrmoyen <- c(enrmoyen, mean(pbc)) # Evolution du pourcentage de prédiction selon C\n        \n}\n\n# Puis on modifie par exemple seqC <- 10^((2):4) afin de chercher à optimiser C",
    "created" : 1415452416037.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3781363990",
    "id" : "7705A681",
    "lastKnownWriteTime" : 1412868756,
    "path" : "E:/TD_AD.R",
    "project_path" : null,
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}